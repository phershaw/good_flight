{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Latent Dirichlet Allocation\n",
    "\n",
    "'Latent Dirichlet Allocation (LDA) is a type of probabilistic topic model that is widely used in natural language processing (NLP) and machine learning to discover abstract topics within a collection of documents. The fundamental idea behind LDA is that documents are represented as random mixtures over latent topics, where each topic is characterized by a distribution over words.'\n",
    "\n",
    "Critically, this technique will look at the type and quantity of words in each review, although it won't take into account the context of the surrounding words.\n",
    "\n",
    "Steps\n",
    "1. Load cleaned data\n",
    "2. Pre-process and tokenize text\n",
    "3. Grid Search to try to maximize coherence in LDA output. Although I maximized coherence, in reality only two topics were the dominant topics in 99.99% of texts. More work is needed in setting the hyperparameters of this model. \n",
    "4. Run model\n",
    "5. Map dominant topics to each row in the data set. This will now be a feature to predict if the passenger will recommend the flight."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Libraries\n",
    "import pandas as pd\n",
    "from tqdm import tqdm \n",
    "\n",
    "import gensim, logging, warnings\n",
    "from gensim.utils import simple_preprocess\n",
    "from gensim import corpora\n",
    "from gensim.models import TfidfModel\n",
    "import gensim.models.ldamodel\n",
    "\n",
    "from collections import Counter\n",
    "import re\n",
    "\n",
    "import nltk\n",
    "from nltk.util import bigrams, trigrams\n",
    "from nltk import download\n",
    "from nltk.corpus import stopwords\n",
    "nltk.download('stopwords')\n",
    "\n",
    "import spacy\n",
    "from spacy.lang.en import English"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the cleaned airline review data\n",
    "df = pd.read_csv('/Users/paulhershaw/brainstation_course/airplane_project/data/airline_reviews_cleaned.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to convert the reviews into a list of words, by setting sentence[2] we look at the review column\n",
    "def convert(sentences):\n",
    "    for sentence in tqdm(sentences):\n",
    "        yield(gensim.utils.simple_preprocess(str(sentence[2]), deacc=True)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# collate values from all rows and place into a list\n",
    "data = df.values.tolist() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# send to function and return list\n",
    "data_words = list(convert(data))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build a list of words and total counts\n",
    "# Download necessary NLTK data\n",
    "download('punkt')\n",
    "\n",
    "# Assuming data_words is a list of lists where each inner list contains words from a sentence\n",
    "data_words_flattened = [word for sentence in data_words for word in sentence]\n",
    "\n",
    "# Count single word occurrences\n",
    "element_counts = Counter(data_words_flattened)\n",
    "\n",
    "\n",
    "# Convert counts to DataFrame for single words, bigrams, and trigrams\n",
    "element_counts_df = pd.DataFrame(element_counts.items(), columns=['Word', 'Count'])\n",
    "\n",
    "\n",
    "# Optional: Check for duplicates in single words (already in the original code)\n",
    "has_duplicates = any(count > 1 for count in element_counts.values())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Export word counts\n",
    "element_counts_df.to_csv('/Users/paulhershaw/brainstation_course/airplane_project/data/element_counts.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assign stopwords to a variable\n",
    "nltk.download('stopwords')\n",
    "stop_words = set(nltk_stopwords.words('english'))  # Use a set for faster lookup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define functions for stopwords and lemmatization\n",
    "\n",
    "'''\n",
    " the process_words function takes a list of texts, removes stopwords, and performs lemmatization using spaCy while filtering tokens based on allowed POS tags. \n",
    "'''\n",
    "\n",
    "\n",
    "def process_words(texts, stop_words=stop_words, allowed_postags=['NOUN', 'ADJ', 'VERB', 'ADV']):\n",
    "    # Initialize spacy 'en' model\n",
    "    nlp = spacy.load('en_core_web_sm', disable=['parser', 'ner'])\n",
    "    \n",
    "    # Remove stopwords\n",
    "    texts = [[word for word in simple_preprocess(str(doc)) if word not in stop_words] for doc in tqdm(texts, desc='Removing stopwords')]\n",
    "    \n",
    "    # Lemmatize\n",
    "    texts_out = []\n",
    "    for sent in tqdm(texts, desc='Lemmatization'):\n",
    "        doc = nlp(\" \".join(sent))\n",
    "        texts_out.append([token.lemma_ for token in doc if token.pos_ in allowed_postags])\n",
    "    return texts_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a list of tokenized text\n",
    "data_ready_single_words = process_words(data_words)  # Process the text data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the dictionary\n",
    "lexicon_single_words = corpora.Dictionary(data_ready_single_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save lexicon to a file\n",
    "lexicon_single_words.save('lexicon_single_words.dict')\n",
    "\n",
    "# Convert data_ready to a DataFrame\n",
    "data_ready_single_words_df = pd.DataFrame(data_ready_single_words)\n",
    "\n",
    "# Save the DataFrame to a CSV file\n",
    "data_ready_single_words_df.to_csv('data_ready_single_words.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### I saved these files, as running this step took time, and it was faster to load from a previously saved file. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use Grid Search to find the best LDA model by coherence score\n",
    "num_topics_range = [6, 12, 24]  \\\n",
    "alpha_range = [0.001, 0.01, 0.1, 1, 'symmetric', 'asymmetric']  \n",
    "\n",
    "# Placeholder for storing the results\n",
    "grid_search_results = []\n",
    "\n",
    "# Function to train LDA model and compute coherence for a given parameter combination\n",
    "def train_lda_and_compute_coherence(num_topics, alpha):\n",
    "    lda_model = LdaModel(\n",
    "        corpus=corpus,\n",
    "        id2word=lexicon_single_words,\n",
    "        num_topics=num_topics,\n",
    "        random_state=42,\n",
    "        update_every=1,\n",
    "        passes=10,\n",
    "        alpha=alpha,\n",
    "        iterations=100,\n",
    "        per_word_topics=True\n",
    "    )\n",
    "    \n",
    "    coherence_model_lda = CoherenceModel(model=lda_model, texts=data_ready_single_words, dictionary=lexicon_single_words, coherence='c_v')\n",
    "    coherence_lda = coherence_model_lda.get_coherence()\n",
    "    \n",
    "    return {\n",
    "        'num_topics': num_topics,\n",
    "        'alpha': alpha,\n",
    "        'coherence': coherence_lda\n",
    "    }\n",
    "\n",
    "# Parallelize the grid search using joblib with a custom progress bar\n",
    "num_cores = 4  # Use up to 4 CPU cores\n",
    "total_iterations = len(num_topics_range) * len(alpha_range)\n",
    "iteration_count = 0\n",
    "\n",
    "for num_topics in num_topics_range:\n",
    "    for alpha in alpha_range:\n",
    "        iteration_count += 1\n",
    "        print(f'Progress: {iteration_count}/{total_iterations}', end='\\r')  # progress bar\n",
    "        \n",
    "        result = train_lda_and_compute_coherence(num_topics, alpha)\n",
    "        grid_search_results.append(result)\n",
    "\n",
    "print(\"\\nBest Model's Params:\", best_result['num_topics'], 'Topics and Alpha:', best_result['alpha'])\n",
    "print(\"Best Model's Coherence Score:\", best_result['coherence'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Baased on Grid Search, train the best LDA model\n",
    "tfidf_single_word = TfidfModel(dictionary=lexicon_single_words, normalize=True)\n",
    "\n",
    "# Create Corpus: Term Document Frequency\n",
    "corpus = [tfidf_single_word[lexicon_single_words.doc2bow(text)] for text in tqdm(data_ready_single_words, desc='Corpus')]\n",
    "\n",
    "# Build LDA model\n",
    "lda_model_single_word = gensim.models.ldamodel.LdaModel(\n",
    "    corpus=corpus,\n",
    "    id2word=lexicon_single_words,\n",
    "    num_topics=24,\n",
    "    random_state=42,\n",
    "    update_every=1,\n",
    "    passes=5,\n",
    "    alpha='asymmetric',\n",
    "    iterations=100,\n",
    "    per_word_topics=True\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LDA Model\n",
    "Based on the results from the Grid Search, I assigned these hyperparameters.\n",
    "\n",
    "Unfortunately, the result is still not ideal. There are two dominant topics, that loosely represent negative and positive reviews. \n",
    "\n",
    "The other topics are simply not dominant.\n",
    "\n",
    "More work is needed. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get dominant topic for each review\n",
    "dominant_topics = []\n",
    "keywords = []\n",
    "for i, row_list in enumerate(lda_model_single_word[corpus]):\n",
    "    row = row_list[0] if lda_model_single_word.per_word_topics else row_list\n",
    "    # Sort the topics by the contribution (weight)\n",
    "    row = sorted(row, key=lambda x: (x[1]), reverse=True)\n",
    "    # Get the dominant topic, its percentage contribution, and keywords\n",
    "    for j, (topic_num, prop_topic) in enumerate(row):\n",
    "        if j == 0:  # => dominant topic\n",
    "            wp = lda_model_single_word.show_topic(topic_num)\n",
    "            topic_keywords = \", \".join([word for word, prop in wp])\n",
    "            dominant_topics.append(int(topic_num))\n",
    "            keywords.append(topic_keywords)\n",
    "        else:\n",
    "            break\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add the dominant topic and keywords to your DataFrame\n",
    "df['Dominant_Topic'] = dominant_topics\n",
    "\n",
    "# View the DataFrame\n",
    "print(df['Dominant_Topic'].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the LDA_Topics column by copying Dominant_Topic\n",
    "df['Dominant_Topic'] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Note almost all the rows land in topic 1 or 0. This si a flaw with the LDA model, and needs further work. \n",
    "topic_counts = df['Dominant_Topic'].value_counts()\n",
    "\n",
    "# Print the counts\n",
    "print(topic_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Update 'Dominant_Topic' directly where its value is 8 or 3, set those to 0\n",
    "df.loc[df['Dominant_Topic'].isin([8, 3]), 'Dominant_Topic'] = 0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Build a dataframe with the index and the dominant topic\n",
    "LDA_Topics = df['Dominant_Topic']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LDA_Topics.to_csv('/Users/paulhershaw/brainstation_course/airplane_project/data/LDA_Topics.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
